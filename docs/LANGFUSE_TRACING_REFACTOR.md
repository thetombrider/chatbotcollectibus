# Refactoring Tracciamento Langfuse - Documentazione Completa

## ğŸ“‹ Sommario

Il tracciamento Langfuse Ã¨ stato completamente refactorizzato seguendo le best practices ufficiali della documentazione Langfuse per garantire:

1. **Gerarchia corretta**: Ogni span/generation Ã¨ collegato al trace padre
2. **Input/Output completi**: Ogni operazione traccia input e output
3. **Propagazione attributi**: `userId` e `sessionId` sono impostati correttamente
4. **Generation objects**: Embeddings e LLM calls usano generation objects invece di span generici

---

## ğŸ”„ Modifiche Principali

### 1. **lib/observability/langfuse.ts** - Nuove API

#### Cambio di paradigma
- âŒ **PRIMA**: Funzioni basate su `traceId` (stringa)
- âœ… **DOPO**: Funzioni basate su `TraceContext` object che include il trace client

#### Nuove funzioni

```typescript
// Interfaccia TraceContext
interface TraceContext {
  traceId: string
  trace: LangfuseTraceClient
  userId: string | null
  sessionId: string
}

// Crea trace con context completo
createChatTrace(
  chatId: string,
  userId: string | null,
  message: string,
  metadata?: Record<string, unknown>
): TraceContext | null

// Crea span figli collegati al trace
createSpan(
  trace: LangfuseTraceClient | LangfuseSpanClient,
  name: string,
  input?: unknown,
  metadata?: Record<string, unknown>
): LangfuseSpanClient | null

// Crea generation per LLM calls
createGeneration(
  parent: LangfuseTraceClient | LangfuseSpanClient,
  name: string,
  model: string,
  input: unknown,
  metadata?: Record<string, unknown>
): LangfuseGenerationClient | null

// Crea generation per embeddings
createEmbeddingGeneration(
  parent: LangfuseTraceClient | LangfuseSpanClient,
  model: string,
  input: string | string[],
  metadata?: Record<string, unknown>
): LangfuseGenerationClient | null

// Gestione ciclo di vita
updateSpan(span, output?, metadata?)
endSpan(span, output?, metadata?)
updateGeneration(generation, output?, usage?, metadata?)
endGeneration(generation, output?, usage?, metadata?)
updateTrace(trace, output?, metadata?)
```

---

### 2. **app/api/chat/route.ts** - Flow principale

#### Struttura del tracciamento

```
chat-request (trace)
â”œâ”€â”€ query-analysis (span)
â”œâ”€â”€ query-enhancement (span)
â”œâ”€â”€ cache-lookup (span)
â”‚   â””â”€â”€ embedding (generation) â† se embedding necessario
â”œâ”€â”€ vector-search (span)
â”‚   â”œâ”€â”€ embedding (generation) â† per query principale
â”‚   â””â”€â”€ comparative-search-1 (span) â† per query comparative
â”‚       â””â”€â”€ embedding (generation)
â”œâ”€â”€ response-generation (span)
â”‚   â””â”€â”€ chat-response (generation) â† LLM call principale
â””â”€â”€ response-processing (span)
```

#### Modifiche implementate

1. **Creazione trace** (linee 343-348):
```typescript
const traceContext = createChatTrace(
  conversationId || 'anonymous',
  userId,
  message,
  { webSearchEnabled, skipCache }
)
// Restituisce TraceContext invece di solo traceId
```

2. **Passaggio context** a tutti gli handler:
```typescript
// Ogni handler riceve traceContext invece di traceId
await handleChatRequest(
  message,
  conversationId || null,
  webSearchEnabled,
  skipCache,
  streamController,
  traceContext // â† TraceContext object completo
)
```

3. **Creazione span per ogni fase**:
```typescript
// Esempio: Query Analysis
const analysisSpan = traceContext 
  ? createSpan(traceContext.trace, 'query-analysis', { message }) 
  : null
const analysis = await analyzeQuery(message)
endSpan(analysisSpan, {
  intent: analysis.intent,
  isMeta: analysis.isMeta,
  // ... altri campi
})
```

4. **Finalizzazione trace** con output completo (linee 300-314):
```typescript
if (traceContext) {
  updateTrace(traceContext.trace, {
    response: processed.content,
    responseLength: processed.content.length,
    sourcesCount: allSources.length,
    cached: false,
  }, {
    analysis: analysis.intent,
    enhancement: enhancement.shouldEnhance,
    searchResultsCount: searchResults.length,
    // ... altre metadata
  })
}
```

---

### 3. **lib/embeddings/openai.ts** - Generation Objects per Embeddings

#### Cambio parametro
- âŒ **PRIMA**: `traceId?: string | null`
- âœ… **DOPO**: `parent?: LangfuseTraceClient | LangfuseSpanClient | null`

#### Implementazione con generation objects

```typescript
export async function generateEmbedding(
  text: string,
  model: string = 'text-embedding-3-large',
  parent?: LangfuseTraceClient | LangfuseSpanClient | null
): Promise<number[]> {
  const normalizedText = normalizeTextForEmbedding(text)
  
  // 1. Crea generation PRIMA della chiamata
  const generation = parent ? createEmbeddingGeneration(
    parent,
    model,
    normalizedText,
    { operation: 'single-embedding', textLength: normalizedText.length }
  ) : null

  try {
    const response = await openai.embeddings.create({...})
    const embedding = response.data[0].embedding
    
    // 2. Aggiorna generation con output
    const usage = response.usage ? { tokens: response.usage.total_tokens } : undefined
    if (generation) {
      updateEmbeddingGeneration(generation, embedding, usage)
      endGeneration(generation)
    }

    return embedding
  } catch (error) {
    // 3. Segna come fallita in caso di errore
    if (generation) {
      endGeneration(generation, undefined, undefined, { 
        error: error.message,
        failed: true 
      })
    }
    throw error
  }
}
```

**Nota**: L'output degli embeddings Ã¨ troppo grande (1536 dimensioni) per essere inviato integralmente. 
`updateEmbeddingGeneration` invia solo metadata (count, dimensions, sample dei primi 10 valori).

---

### 4. **app/api/chat/handlers/response-handler.ts** - LLM Generation

#### Cambio ResponseContext
```typescript
export interface ResponseContext {
  // ... altri campi
  traceContext?: TraceContext | null  // â† invece di traceId
}
```

#### Tracciamento LLM call (linee 188-214)

```typescript
if (context.traceContext && fullResponse) {
  // Crea generation per la chiamata LLM principale
  const generation = createGeneration(
    context.traceContext.trace,
    'chat-response',
    'openrouter/google/gemini-2.5-flash',
    messages,  // Input: array di messaggi
    {
      operation: 'chat-response',
      messageLength: message.length,
      hasContext: contextText !== null,
      contextLength: contextText?.length || 0,
      // ... altre metadata
    }
  )

  // Finalizza generation con output
  endGeneration(
    generation,
    fullResponse,  // Output: risposta completa
    undefined,     // Usage non disponibile da Mastra
    { responseLength: fullResponse.length }
  )
}
```

---

### 5. **app/api/chat/handlers/search-handler.ts** - Multi-Query Tracing

#### Query comparative con span figli

```typescript
export async function performMultiQuerySearch(
  terms: string[],
  originalQuery: string,
  originalEmbedding: number[],
  articleNumber?: number,
  traceContext?: TraceContext | null,
  parentSpan?: ReturnType<typeof createSpan> | null
): Promise<SearchResult[]> {
  
  const searchPromises = terms.map(async (term, index) => {
    // 1. Crea span per questo termine specifico
    const termSpan = (parentSpan && traceContext) 
      ? createSpan(parentSpan, `comparative-search-${index + 1}`, {
          term,
          index: index + 1,
          totalTerms: terms.length,
        }) 
      : null

    try {
      // 2. Embedding con generation object figlio
      const targetedEmbedding = await generateEmbedding(
        term, 
        'text-embedding-3-large', 
        termSpan || (traceContext ? traceContext.trace : null)
      )
      
      // 3. Ricerca
      const results = await hybridSearch(...)
      
      // 4. Finalizza span con risultati
      endSpan(termSpan, {
        resultsCount: results.length,
        bestSimilarity: results[0]?.similarity || 0,
      })
      
      return results
    } catch (err) {
      // 5. Segna come fallito
      endSpan(termSpan, undefined, {
        error: err.message,
        failed: true,
      })
      return []
    }
  })
  
  // ...
}
```

---

### 6. **app/api/chat/handlers/cache-handler.ts** - Metadata Aggiuntive

Modifiche minime: aggiunto parametro `traceContext` per futuri miglioramenti e log piÃ¹ dettagliati.

```typescript
export async function lookupCache(
  query: string,
  queryEmbedding: number[],
  skipCache: boolean = false,
  traceContext?: TraceContext | null  // â† nuovo parametro
): Promise<CacheResult>
```

---

## ğŸ¯ Benefici del Refactoring

### 1. **Gerarchia Corretta**
Tutti gli span sono correttamente collegati al trace padre:
- âœ… Nessuno span "orfano"
- âœ… Struttura ad albero chiara e navigabile
- âœ… Facile identificare quale operazione Ã¨ figlia di quale

### 2. **Input/Output Completi**
Ogni operazione traccia input e output:
- âœ… Query analysis: input=message, output=analysis result
- âœ… Embeddings: input=text, output=metadata (dimensions, count, sample)
- âœ… Vector search: input=query params, output=results stats
- âœ… LLM generation: input=messages, output=response

### 3. **Metadata Corrette**
- âœ… `userId` e `sessionId` impostati a livello trace
- âœ… Metadata specifiche per ogni tipo di operazione
- âœ… Metadata di errore quando operazioni falliscono

### 4. **Generation Objects**
Embeddings e LLM calls usano generation objects invece di span:
- âœ… Calcolo automatico dei costi (quando usage disponibile)
- âœ… Metriche specifiche per modelli LLM
- âœ… Aggregazioni corrette per tipo di operazione

### 5. **Error Handling**
Ogni operazione gestisce correttamente gli errori:
- âœ… Span/generation marcati come falliti
- âœ… Metadata di errore incluse
- âœ… Stack trace preserved per debugging

---

## ğŸ“Š Struttura Completa di un Trace

Esempio di struttura completa in Langfuse:

```
ğŸ“Š chat-request (trace)
â”‚  input: { message: "Confronta GDPR e NIS2" }
â”‚  output: { response: "...", responseLength: 1234, sourcesCount: 5 }
â”‚  metadata: { userId: "user-123", sessionId: "chat-456", ... }
â”‚
â”œâ”€ ğŸ” query-analysis (span)
â”‚  â”‚  input: { message: "Confronta GDPR e NIS2" }
â”‚  â”‚  output: { intent: "comparative", isComparative: true, ... }
â”‚  
â”œâ”€ âœ¨ query-enhancement (span)
â”‚  â”‚  input: { original: "...", analysis: {...} }
â”‚  â”‚  output: { enhanced: "...", shouldEnhance: true }
â”‚  
â”œâ”€ ğŸ’¾ cache-lookup (span)
â”‚  â”‚  input: { query: "..." }
â”‚  â”‚  output: { cached: false }
â”‚  â”‚
â”‚  â””â”€ ğŸ¤– embedding (generation)
â”‚     â”‚  model: "text-embedding-3-large"
â”‚     â”‚  input: ["Confronta GDPR e NIS2"]
â”‚     â”‚  output: { type: "embedding", count: 1, dimensions: 1536, ... }
â”‚     â”‚  usage: { promptTokens: 15, totalTokens: 15 }
â”‚  
â”œâ”€ ğŸ” vector-search (span)
â”‚  â”‚  input: { query: "...", isComparative: true, terms: ["GDPR", "NIS2"] }
â”‚  â”‚  output: { totalResults: 15, relevantResults: 10, avgSimilarity: 0.753 }
â”‚  â”‚
â”‚  â”œâ”€ ğŸ” comparative-search-1 (span)
â”‚  â”‚  â”‚  input: { term: "GDPR", index: 1, totalTerms: 2 }
â”‚  â”‚  â”‚  output: { resultsCount: 8, bestSimilarity: 0.812 }
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ ğŸ¤– embedding (generation)
â”‚  â”‚     â”‚  model: "text-embedding-3-large"
â”‚  â”‚     â”‚  input: ["GDPR"]
â”‚  â”‚     â”‚  output: { type: "embedding", ... }
â”‚  â”‚     â”‚  usage: { promptTokens: 5, totalTokens: 5 }
â”‚  â”‚
â”‚  â””â”€ ğŸ” comparative-search-2 (span)
â”‚     â”‚  input: { term: "NIS2", index: 2, totalTerms: 2 }
â”‚     â”‚  output: { resultsCount: 7, bestSimilarity: 0.798 }
â”‚     â”‚
â”‚     â””â”€ ğŸ¤– embedding (generation)
â”‚        â”‚  model: "text-embedding-3-large"
â”‚        â”‚  input: ["NIS2"]
â”‚        â”‚  output: { type: "embedding", ... }
â”‚        â”‚  usage: { promptTokens: 5, totalTokens: 5 }
â”‚  
â”œâ”€ ğŸ¨ response-generation (span)
â”‚  â”‚  input: { query: "...", contextLength: 15234, sourcesCount: 10 }
â”‚  â”‚  output: { responseLength: 1234, truncated: "..." }
â”‚  â”‚
â”‚  â””â”€ ğŸ¤– chat-response (generation)
â”‚     â”‚  model: "openrouter/google/gemini-2.5-flash"
â”‚     â”‚  input: [{ role: "system", content: "..." }, { role: "user", content: "..." }]
â”‚     â”‚  output: "La GDPR (General Data Protection Regulation)..."
â”‚     â”‚  usage: undefined (non disponibile da Mastra)
â”‚     â”‚  metadata: { operation: "chat-response", responseLength: 1234, ... }
â”‚  
â””â”€ ğŸ“ response-processing (span)
   â”‚  input: { responseLength: 1234, webResultsCount: 0, metaDocumentsCount: 0 }
   â”‚  output: { processedLength: 1234, sourcesCount: 5, webSourcesCount: 0 }
```

---

## ğŸš€ Prossimi Passi

### Immediate (giÃ  implementate)
- âœ… Refactoring completo langfuse.ts
- âœ… Aggiornamento route.ts con TraceContext
- âœ… Generation objects per embeddings
- âœ… Generation objects per LLM calls
- âœ… Span figli per query comparative
- âœ… Error handling migliorato

### Future (opzionali)
- ğŸ”œ **Propagazione attributi con `propagateAttributes()`**: 
  - Secondo la documentazione Langfuse, userId/sessionId dovrebbero essere propagati a TUTTE le osservazioni
  - Questo permetterÃ  metriche per utente/sessione a livello di singola osservazione
  - Attualmente sono solo a livello trace, ma funzionano comunque

- ğŸ”œ **Intercettare token usage da Mastra**:
  - Mastra agent stream non espone direttamente usage tokens
  - Possibile intercettare dal response stream per tracciare costi accurati

- ğŸ”œ **Flush automatico in Vercel**:
  - Aggiungere `flushLangfuse()` alla fine delle richieste
  - Importante per ambienti serverless per garantire invio dati

---

## ğŸ“š Riferimenti

- [Langfuse TypeScript SDK v4 Documentation](https://langfuse.com/docs/observability/sdk/typescript/overview)
- [Best Practices for Tracing](https://langfuse.com/docs/observability/sdk/typescript/instrumentation)
- [Trace Context Propagation](https://langfuse.com/docs/observability/features/sessions)
- [Generation Objects for LLM Calls](https://langfuse.com/docs/observability/features/observation-types)

---

## âœ… Checklist Completamento

- [x] Refactoring lib/observability/langfuse.ts
- [x] Aggiornamento app/api/chat/route.ts
- [x] Modifica lib/embeddings/openai.ts
- [x] Aggiornamento handlers (cache, search, response)
- [x] Rimozione funzioni obsolete
- [x] Validazione linter (0 errori)
- [x] Documentazione completa

---

**Data refactoring**: Novembre 2025  
**Versione Langfuse SDK**: v3 (Node.js)  
**Testato**: âœ… No linter errors

